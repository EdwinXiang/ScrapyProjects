　　在今年早些时候，展示了他们在Nervana神经网络处理器方面的进展，Nervana芯片在机器学习和AI领域内大大超过了普通芯片，如今他们又发布了Nervana的最新信息，并表示正努力将其推向市场。　　和传统的处理器不同，Nervana特别强调矩阵乘法和卷积计算，这是深度学习用到最多的场景，需要大量的内存和算数运算，传统的芯片结构已经不能适用。为了减少读取外部存储的操作，保证内存带宽的够用，Nervana芯片取消了常见的Cache，改由软件直接负责芯片内存的管理，芯片本地内存模块增加到2MB，每个Nervana芯片约有30M空间，可以一次加载更多的外部数据。　　为了进一步的节省内存空间和带宽，英特尔还专为人工智能设计了一种新的数据类型Flexpoint。由于神经网络对于数据的杂音容忍度较高，低精度的Flexpoint类型可以在16bit的存储上，实现近似于32bit浮点操作的效果，节省了一半空间，让处理器和内存带宽的使用效率翻倍。　　Nervana芯片还设计有片外互联通道，支持Tbit级别的高速双向资料传输，多个处理器可以组成一个更大的神经网络系统来执行单项任务工作。　　英特尔在去年收购了深度学习芯片厂商Nervana Systems，意图在AI领域后来追上，不过其它厂商也没干等着，NVIDIA有针对AI应用所设计的 V100，而谷歌为数据中心应用打造了一款Tensor Processing Unit （TPU）芯片，IBM也发表了名为True North的仿神经型态芯片。各家厂商都在机器学习和AI领域加大投入，相信不用太久，这些新技术都会出现在普通人的生活中。